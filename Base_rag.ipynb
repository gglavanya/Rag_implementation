{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9679e2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Loading the PDF file using PyPDFLoader\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "\n",
    "def load_pdf(file_path: str):\n",
    "    loader = PyPDFLoader(file_path)\n",
    "    pages = loader.load()\n",
    "\n",
    "    print(f\"Total Pages Found: {len(pages)}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Content of Page 1 (First 500 chars):\\n{pages[0].page_content[:500]}\")\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Metadata of Page 1: {pages[0].metadata}\")\n",
    "\n",
    "    return pages\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2bbb39ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Pages Found: 8\n",
      "------------------------------\n",
      "Content of Page 1 (First 500 chars):\n",
      "Candidate:  Gurudevi  Lavanya  Gopisetty  \n",
      " \n",
      "1.  Personal  &  Academic  Overview  \n",
      "‚óè  Full  Name:  Gurudevi  Lavanya  Gopisetty  ‚óè  Date  of  Birth  (DOB):  june  10  ,  1999  ‚óè  Location:  Long  Beach,  California,  USA  ‚óè  Email:  gglavanya06@gmail.com  ‚óè  Phone:  +1  (669)  306-3851  \n",
      "Education  \n",
      "‚óè  Master‚Äôs  Degree:  M.S.  in  Computer  Science  ‚óè  University:  California  State  University  ‚óè  Expected  Graduation:  December  2025  ‚óè  Cumulative  GPA:  3.909  /  4.0  \n",
      " \n",
      "2.  Professional  Ro\n",
      "------------------------------\n",
      "Metadata of Page 1: {'producer': 'Skia/PDF m145 Google Docs Renderer', 'creator': 'PyPDF', 'creationdate': '', 'title': 'Rag_docs', 'source': 'Rag_docs.pdf', 'total_pages': 8, 'page': 0, 'page_label': '1'}\n"
     ]
    }
   ],
   "source": [
    "my_pages=load_pdf('Rag_docs.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "3475951d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating the chunks to give to the vector database\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "def create_chunks(text):\n",
    "\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=500,\n",
    "        chunk_overlap=50,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n",
    "    )\n",
    "    chunks = splitter.split_documents(text)\n",
    "    \n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cc433c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Created 20 chunks.\n",
      "First chunk preview: Candidate:  Gurudevi  Lavanya  Gopisetty  \n",
      " \n",
      "1.  Personal  &  Academic  Overview  \n",
      "‚óè  Full  Name:  G\n"
     ]
    }
   ],
   "source": [
    "my_chunks=create_chunks(my_pages)\n",
    "\n",
    "\n",
    "print(f\"‚úÖ Created {len(my_chunks)} chunks.\")\n",
    "print(f\"First chunk preview: {my_chunks[0].page_content[:100]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7bfacb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting the chunks into embbedings into numerical form, so it can understand\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "\n",
    "def get_embeddings_model():\n",
    "    model_name= \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "    model_kwargs = {\"device\": \"cpu\"}\n",
    "    encode_kwargs={'normalize_embeddings': False}\n",
    "\n",
    "    return HuggingFaceEmbeddings(model_name=model_name, model_kwargs=model_kwargs, encode_kwargs=encode_kwargs) \n",
    "\n",
    "embed_model= get_embeddings_model()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fb450007",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Embedding Successful!\n",
      "Vector Length: 384\n",
      "First 5 numbers of the vector: [-0.021258514374494553, 0.03264220803976059, -0.05218735337257385, -0.006472242064774036, 0.06284356117248535]\n"
     ]
    }
   ],
   "source": [
    "test_text = \"This is a test to see what a vector looks like.\"\n",
    "vector = embed_model.embed_query(test_text)\n",
    "\n",
    "print(f\"‚úÖ Embedding Successful!\")\n",
    "print(f\"Vector Length: {len(vector)}\") # Should be 384 for MiniLM\n",
    "print(f\"First 5 numbers of the vector: {vector[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e4d2d17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now storing the embeddings into a vector database where it can be stored according to its value and positions and can get the similarity by distance calculation\n",
    "from langchain_chroma import Chroma \n",
    "def create_vector_db(chunks, embed_model):\n",
    "    vector_db = Chroma.from_documents(\n",
    "        documents=chunks,\n",
    "        embedding=embed_model,\n",
    "        persist_directory=\"./vector_db\"\n",
    "    )\n",
    "    return vector_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7ff51c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- TOP SEARCH RESULT ---\n",
      "Content: Candidate:  Gurudevi  Lavanya  Gopisetty  \n",
      " \n",
      "1.  Personal  &  Academic  Overview  \n",
      "‚óè  Full  Name:  Gurudevi  Lavanya  Gopisetty  ‚óè  Date  of  Birth  (DOB):  june  10  ,  1999  ‚óè  Location:  Long  Beac...\n",
      "Metadata: {'producer': 'Skia/PDF m145 Google Docs Renderer', 'page_label': '1', 'creator': 'PyPDF', 'source': 'Rag_docs.pdf', 'creationdate': '', 'page': 0, 'title': 'Rag_docs', 'total_pages': 8}\n"
     ]
    }
   ],
   "source": [
    "vector_db = create_vector_db(my_chunks, embed_model)\n",
    "\n",
    "# --- INSPECTION: The \"Similarity Search\" Test ---\n",
    "# Let's see if it can find the right chunk without an LLM\n",
    "query = \"What is the name of the person?\" # Change this to a topic in your PDF\n",
    "search_results = vector_db.similarity_search(query, k=1) # Get top 2 matches\n",
    "\n",
    "print(\"\\n--- TOP SEARCH RESULT ---\")\n",
    "print(f\"Content: {search_results[0].page_content[:200]}...\")\n",
    "print(f\"Metadata: {search_results[0].metadata}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fe3393c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "def get_answer(question, vector_db):\n",
    "    llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "    # 1. Define the prompt\n",
    "    template = template = \"\"\"\n",
    "    You are a professional and friendly career assistant. \n",
    "    Your goal is to answer questions about a candidate's skills based on their resume/profile data.\n",
    "    your goal is to answer the question as if the candidate answers to the requiterer.\n",
    "\n",
    "    RULES:\n",
    "    1. Do NOT say \"Based on the context\" or \"According to the document.\"\n",
    "    2. Speak as if you already know the candidate well.\n",
    "    3. Use a friendly, conversational, and professional tone.\n",
    "    4. If the information isn't there, politely say you aren't sure about that specific detail.\n",
    "    5. Keep it concise but enthusiastic.\n",
    "    6. If you don't have enough information to answer, ask for clarification.\n",
    "    7. if the question is ask about a job/role she fits in, i want you to answer accordingly , and also put the skills and relevant projects worked on.\n",
    "    8. start the sentence like  \"Lavanya did this, Lavanya did that, Lavanya is a professional in this field, Lavanya has worked on this project, Lavanya is skilled in this area.\"\n",
    "    9. if the question is ask about a job/role she fits in, i want you to answer accordingly , and also put the skills and relevant projects worked on.\n",
    "\n",
    "    Candidate Information:\n",
    "    {context}\n",
    "    \n",
    "    User Question: {question}\n",
    "    Friendly Response:\n",
    "   \n",
    "    \"\"\"\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "    # 2. Build the chain using the \"Pipe\" | operator\n",
    "    # This says: Get context -> Pass to prompt -> Pass to LLM -> Parse as string\n",
    "    chain = (\n",
    "        {\"context\": vector_db.as_retriever(), \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 3. Run it\n",
    "    return chain.invoke(question)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "2d79687d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ AI ANSWER:\n",
      "Lavanya is very skilled in machine learning, having demonstrated a strong applied understanding of ML fundamentals and mathematical intuition behind models. She has worked on several projects that showcase her expertise, including developing predictive models for real-world data. \n",
      "\n",
      "Some relevant skills and projects she's worked on include: building neural networks for image classification, implementing decision trees for regression analysis, and optimizing machine learning algorithms using hyperparameter tuning techniques.\n",
      "\n",
      "Lavanya is also well-versed in the theoretical aspects of machine learning, having learned from her academic background and practical experience. She has a solid grasp of data structures and algorithms, operating systems, finite automata, and deep learning concepts.\n"
     ]
    }
   ],
   "source": [
    "answer = get_answer(\"is she good in machine learning \" \\\n",
    "\"\", vector_db)\n",
    "print(f\"ü§ñ AI ANSWER:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "a692cb45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#implementation of cache for storing the results of the query\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "CACHE_PATH = \"./my_semantic_cache\"\n",
    "SIMILARITY_THRESHOLD = 0.8\n",
    "TTL_seconds = 60 * 60 * 24 * 7 # 1 week\n",
    "Max_CACHE_SIZE = 100\n",
    "\n",
    "def check_cache(question,embed_model):\n",
    "    cache_db= Chroma(\n",
    "        embedding_function=embed_model,\n",
    "        persist_directory=CACHE_PATH,\n",
    "        collection_name=\"qa_cache\"\n",
    "    )\n",
    "\n",
    "    #to search for the query in the cache\n",
    "    results = cache_db.similarity_search_with_relevance_scores(question, k=1)\n",
    "    if results and results[0][1] >= 0.90:\n",
    "        doc, score = results[0]\n",
    "        age = time.time() - doc.metadata.get(\"timestamp\",0)\n",
    "        if age > TTL_seconds:\n",
    "            print(f\"Cache expired after {TTL_seconds} seconds.\")\n",
    "            return None,0\n",
    "        \n",
    "        #LRU update\n",
    "        doc.metadata[\"hit_count\"] += 1\n",
    "        doc.metadata[\"timestamp\"] = time.time()\n",
    "\n",
    "\n",
    "        doc,score = results[0]\n",
    "\n",
    "        if score > SIMILARITY_THRESHOLD:\n",
    "            print(f\"‚úÖ Cache Hit! Score: {score}\")\n",
    "            return doc.metadata[\"answer\"],score\n",
    "    \n",
    "    return None,0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "76c8d833",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import uuid\n",
    "def save_to_cache(question, answer, embed_model):\n",
    "    cache_db= Chroma(\n",
    "        embedding_function=embed_model,\n",
    "        persist_directory=CACHE_PATH,\n",
    "        collection_name=\"qa_cache\"\n",
    "    )\n",
    "\n",
    "    unique_id = str(uuid.uuid4())\n",
    "    cache_db.add_texts(\n",
    "        texts=[question],\n",
    "        ids=[unique_id],\n",
    "        metadatas=[{\"answer\": answer,\n",
    "                    \"timestamp\": time.time(),\n",
    "                    \"hit_count\": 0}]\n",
    "    )\n",
    "    print(f\"üíæ Saved to cache with ID: {unique_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "a639a121",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup_cache_by_id(cache_db, max_size=100):\n",
    "    # 1. Get all IDs and timestamps from the cache\n",
    "    data = cache_db.get(include=['metadatas'])\n",
    "    ids = data['ids']\n",
    "    metadatas = data['metadatas']\n",
    "    \n",
    "    # 2. If we are over the limit, find the oldest ones to delete\n",
    "    if len(ids) > max_size:\n",
    "        # Create a list of (id, timestamp) tuples\n",
    "        entries = []\n",
    "        for i in range(len(ids)):\n",
    "            entries.append((ids[i], metadatas[i]['timestamp']))\n",
    "        \n",
    "        # Sort by timestamp (oldest first)\n",
    "        entries.sort(key=lambda x: x[1])\n",
    "        \n",
    "        # 3. Identify how many to delete\n",
    "        num_to_delete = len(ids) - max_size\n",
    "        ids_to_remove = [e[0] for e in entries[:num_to_delete]]\n",
    "        \n",
    "        # 4. EXECUTE DELETION\n",
    "        cache_db.delete(ids=ids_to_remove)\n",
    "        print(f\"üßπ Cleaned up {len(ids_to_remove)} old cache entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7ea8e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_answer_with_cache(question, vector_db, embed_model):\n",
    "    cached_answer, score = check_cache(question, embed_model)\n",
    "    if cached_answer:\n",
    "        return f\"‚ö° [Cache Hit - {score:.2f}] {cached_answer}\"\n",
    "\n",
    "    answer = get_answer(question, vector_db)\n",
    "    save_to_cache(question, answer, embed_model)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "da8d0b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cache expired after 604800 seconds.\n",
      "üíæ Saved to cache with ID: 7b756168-a3ca-47c9-9db5-a8b05705f5e9\n",
      "ü§ñ AI ANSWER:\n",
      "Lavanya has a strong applied understanding of machine learning fundamentals, which includes mathematical intuition behind models, best practices for real-world ML systems. She's also shown a solid grasp of key concepts in data structures and algorithms, operating systems, finite automata, machine learning, deep learning, artificial intelligence, probability & statistics.\n",
      "\n",
      "She has demonstrated practical experience with these topics by working on relevant projects, such as developing predictive models using Python libraries like scikit-learn and TensorFlow. Her understanding is strong, but I'd love to see more hands-on experience in this area.\n"
     ]
    }
   ],
   "source": [
    "question = \"is she good in machine learning\"\n",
    "answer = get_answer_with_cache(question, vector_db, embed_model)\n",
    "print(f\"ü§ñ AI ANSWER:\\n{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "7cdd122d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "import hashlib\n",
    "import time\n",
    "# Constants\n",
    "SIMILARITY_THRESHOLD = 0.90\n",
    "\n",
    "def run_ai_pipeline(user_query, vector_db, cache_db, rag_chain):\n",
    "    \"\"\"\n",
    "    The main workflow: Semantic Cache -> RAG -> Update Cache\n",
    "    \"\"\"\n",
    "    # 1. Check Semantic Cache\n",
    "    # similarity_search_with_relevance_scores returns (Document, Score)\n",
    "    cache_results = cache_db.similarity_search_with_relevance_scores(user_query, k=1)\n",
    "    \n",
    "    if cache_results and cache_results[0][1] >= SIMILARITY_THRESHOLD:\n",
    "        doc, score = cache_results[0]\n",
    "        return f\"‚ö° [CACHE HIT - {score:.2f}]\\n{doc.metadata['answer']}\"\n",
    "\n",
    "    # 2. Cache Miss: Run the RAG Chain\n",
    "    print(\"üê¢ Cache Miss. Thinking...\")\n",
    "    response = rag_chain.invoke(user_query)\n",
    "    \n",
    "    # Handle both dictionary and string outputs from the chain\n",
    "    answer = response if isinstance(response, str) else response.get(\"answer\", \"I'm sorry, I couldn't find an answer.\")\n",
    "\n",
    "    q_id = hashlib.md5(user_query.lower().encode()).hexdigest()\n",
    "    # 3. Update Cache for next time\n",
    "    cache_db.add_texts(\n",
    "        texts=[user_query],\n",
    "        ids=[q_id],\n",
    "        metadatas=[{\"answer\": answer}]\n",
    "    )\n",
    "\n",
    "    #4 The cleanup\n",
    "    cleanup_cache_by_id(cache_db, max_size=100)\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e01560",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready! Type 'exit' to stop.\n",
      "üê¢ Cache Miss. Thinking...\n",
      "\n",
      "ü§ñ Assistant: Lavanya has a solid foundation in machine learning fundamentals, with a strong understanding of data structures and algorithms. She's also demonstrated mathematical intuition behind models and has applied this knowledge to real-world ML systems.\n",
      "\n",
      "Lavanya has worked on several projects that showcase her skills in machine learning, including developing and testing predictive models using scikit-learn and TensorFlow. Her experience with deep learning frameworks like Keras and PyTorch has been particularly valuable in terms of practical impact and application.\n",
      "\n",
      "I'm confident that Lavanya's expertise in machine learning would be a strong asset for any organization looking to leverage ML capabilities. With her academic knowledge, mathematical intuition, and practical experience working on projects, she would be well-suited for roles in AI research and development, data science, or related fields.\n",
      "\n",
      "ü§ñ Assistant: ‚ö° [CACHE HIT - 1.00]\n",
      "Lavanya did not indicate her current or past affiliation with any specific university, so I'm unsure about that detail.\n",
      "\n",
      "Lavanya did mention that she received a Master's Degree from California State University, but unfortunately, that does not include the exact name of the institution. If you're looking for more information on her educational background, I can try to dig deeper or ask follow-up questions to help clarify!\n",
      "üê¢ Cache Miss. Thinking...\n",
      "\n",
      "ü§ñ Assistant: Lavanya did have a cumulative GPA, Lavanya did not get to it yet, but I know that she has worked on projects related to academic improvement and understanding of her own strengths.\n",
      "\n",
      "Lavanya did take courses in data analysis and statistics, which is one area where her skills are relevant. She also participated in hackathons and coding competitions, demonstrating her ability to work under pressure and solve complex problems.\n",
      "\n",
      "She has a Master's Degree in Computer Science from California State University, and her experience as a software engineer at various companies would be beneficial for this role.\n",
      "üê¢ Cache Miss. Thinking...\n",
      "\n",
      "ü§ñ Assistant: Lavanya did have experience working on sales pilots for various AI solutions, which is a key aspect of the Pre and Post Sales Pilots role she fits in well.\n",
      "\n",
      "Lavanya has developed several AI-powered sales tools to help companies validate their proposals with pre-pilot and post-pilot scenarios. She collaborated closely with cross-functional teams, including engineers and marketers, to ensure that these pilots aligned with customer requirements and expectations while adhering to best practices and quality standards. Her expertise in machine learning, deep learning model development, computer vision, NLP systems, generative AI experimentation, data analysis, predictive modeling, software engineering with algorithmic foundations, automation testing, and AI-driven system design has proven her adaptability and reliability.\n",
      "\n",
      "Lavanya is confident that she can apply these skills to a Pre and Post Sales Pilots role, where she would be working on developing pilots, collaborating with teams, and continuously monitoring the performance of AI solutions. Her experience in solution optimization and performance enhancement, collaboration and project management, communication with stakeholders, and industry trends would all contribute to her success in this position.\n",
      "\n",
      "She is particularly interested in leveraging her knowledge of foundation models and large language models to drive efficiency, accuracy, and speed in AI solution development.\n",
      "Stopping session. Goodbye!\n"
     ]
    }
   ],
   "source": [
    "# --- ONE-TIME SETUP ---\n",
    "# (Assumes you have already defined embed_model, vector_db, and llm in previous cells)\n",
    "import time\n",
    "# 1. Initialize Cache DB\n",
    "cache_db = Chroma(\n",
    "    persist_directory=\"./my_semantic_cache\",\n",
    "    embedding_function=embed_model,\n",
    "    collection_name=\"qa_cache\"\n",
    ")\n",
    "#using llm again\n",
    "llm = ChatOllama(model=\"llama3.2:1b\")\n",
    "\n",
    "# 2. Create the RAG Chain\n",
    "prompt = ChatPromptTemplate.from_template(template  = \"\"\"\n",
    "    You are a professional and friendly career assistant. \n",
    "    Your goal is to answer questions about a candidate's skills based on their resume/profile data.\n",
    "    your goal is to answer the question as if the candidate answers to the requiterer.\n",
    "\n",
    "    RULES:\n",
    "    1. Do NOT say \"Based on the context\" or \"According to the document.\"\n",
    "    2. Speak as if you already know the candidate well.\n",
    "    3. Use a friendly, conversational, and professional tone.\n",
    "    4. If the information isn't there, politely say you aren't sure about that specific detail.\n",
    "    5. Keep it concise but enthusiastic.\n",
    "    6. If you don't have enough information to answer, ask for clarification.\n",
    "    7. if the question is ask about a job/role she fits in, i want you to answer accordingly , and also put the skills and relevant projects worked on.\n",
    "    8. start the sentence like  \"Lavanya did this, Lavanya did that, Lavanya is a professional in this field, Lavanya has worked on this project, Lavanya is skilled in this area.\"\n",
    "    9. Make sure the answer should be in 3 sentences and should be relevant to the question.\n",
    "    10. Also Metion the skills when needed.\n",
    "    11. if the user asks is she fit in this job , mention the relevant projects she did while answering the project.                                   \n",
    "                                          \n",
    "                                          \n",
    "\n",
    "    Candidate Information:\n",
    "    {context}\n",
    "    \n",
    "    User Question: {question}\n",
    "    Friendly Response:\n",
    "   \n",
    "    \"\"\")\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": vector_db.as_retriever(search_kwargs={\"k\": 3}), \"question\": RunnablePassthrough()}\n",
    "    | prompt | llm | StrOutputParser()\n",
    ")\n",
    "\n",
    "# --- THE MAIN LOOP ---\n",
    "print(\"Ready! Type 'exit' to stop.\")\n",
    "while True:\n",
    "    time.sleep(0.5)  # To prevent rapid fire inputs\n",
    "    query = input(\"\\nAsk about the candidate: \")\n",
    "    \n",
    "    if query.lower() in ['exit', 'quit', 'stop']:\n",
    "        print(\"Stopping session. Goodbye!\")\n",
    "        break\n",
    "        \n",
    "    final_response = run_ai_pipeline(query, vector_db, cache_db, rag_chain)\n",
    "    print(f\"\\nü§ñ Assistant: {final_response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ec86a62",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
